{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11026981,"sourceType":"datasetVersion","datasetId":6867024}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip  install pm4py\n!pip install python-Levenshtein","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T02:00:57.872954Z","iopub.execute_input":"2025-05-02T02:00:57.873139Z","iopub.status.idle":"2025-05-02T02:01:11.963960Z","shell.execute_reply.started":"2025-05-02T02:00:57.873119Z","shell.execute_reply":"2025-05-02T02:01:11.963117Z"}},"outputs":[{"name":"stdout","text":"Collecting pm4py\n  Downloading pm4py-2.7.15.1-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pm4py) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pm4py) (2.2.3)\nCollecting deprecation (from pm4py)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pm4py) (3.4.2)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from pm4py) (0.20.3)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from pm4py) (0.45.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pm4py) (75.1.0)\nCollecting intervaltree (from pm4py)\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pm4py) (5.3.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pm4py) (3.7.5)\nRequirement already satisfied: pydotplus in /usr/local/lib/python3.10/dist-packages (from pm4py) (2.0.2)\nRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from pm4py) (2025.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pm4py) (1.13.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pm4py) (4.67.1)\nRequirement already satisfied: cvxopt in /usr/local/lib/python3.10/dist-packages (from pm4py) (1.3.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation->pm4py) (24.2)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree->pm4py) (2.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pm4py) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pm4py) (2.4.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->pm4py) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pm4py) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pm4py) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pm4py) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pm4py) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pm4py) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pm4py) (2024.2.0)\nDownloading pm4py-2.7.15.1-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: intervaltree\n  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=d82a84e873a8ba5baf9e801ed04f17153288794438646fc9fe1d5253525668f8\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built intervaltree\nInstalling collected packages: intervaltree, deprecation, pm4py\nSuccessfully installed deprecation-2.1.0 intervaltree-3.1.0 pm4py-2.7.15.1\nCollecting python-Levenshtein\n  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting Levenshtein==0.27.1 (from python-Levenshtein)\n  Downloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.5/161.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\nSuccessfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pm4py\nimport numpy as np\nimport cupy as cp\nfrom sklearn.cluster import DBSCAN as SklearnDBSCAN\nfrom sklearn.manifold import MDS\nimport json\nfrom pm4py.objects.log.importer.xes import importer as xes_importer\nfrom pm4py.algo.discovery.alpha import algorithm as alpha_miner\nfrom collections import Counter\nimport time\nimport math\nfrom numba import cuda\n\n# Multi-GPU setup\nn_gpus = cp.cuda.runtime.getDeviceCount()\nprint(f\"Running: Using {n_gpus} GPU(s)...\")\n\n# Load .xes file\nlog_path = \"/kaggle/input/event-log/RequestForPayment.xes\"\nprint(\"Running: Loading .xes file...\")\nstart_time = time.time()\nlog = xes_importer.apply(log_path)\ntraces = [tuple(event[\"concept:name\"] for event in trace) for trace in log]\nunique_activities = sorted(set().union(*[set(trace) for trace in traces]))\nn_traces = len(traces)\nprint(f\"Completed loading: {time.time() - start_time:.2f}s (Traces: {n_traces})\")\n\n# A4: Generic Edit Distance\ndef get_3grams(traces, activities):\n    activity_to_idx = {a: i for i, a in enumerate(activities)}\n    g3_counts = cp.zeros((len(activities), len(activities), len(activities)), dtype=cp.int32)\n    for trace in traces:\n        for i in range(len(trace) - 2):\n            x, a, y = trace[i:i+3]\n            g3_counts[activity_to_idx[x], activity_to_idx[a], activity_to_idx[y]] += 1\n    return g3_counts\n\ndef compute_substitution_scores_multi_gpu(traces, activities):\n    print(\"Running: Computing Substitution Scores (Algorithm 1)...\")\n    start_time = time.time()\n    n_activities = len(activities)\n    g3_counts = get_3grams(traces, activities)\n    Xa = [set() for _ in range(n_activities)]\n    for x_idx in range(n_activities):\n        for a_idx in range(n_activities):\n            for y_idx in range(n_activities):\n                if g3_counts[x_idx, a_idx, y_idx] > 0:\n                    Xa[a_idx].add((x_idx, y_idx))\n    C = cp.zeros((n_activities, n_activities), dtype=cp.float32)\n    for a_idx in range(n_activities):\n        for b_idx in range(n_activities):\n            if a_idx == b_idx:\n                counts = g3_counts[:, a_idx, :]\n                C[a_idx, a_idx] = cp.sum(counts * (counts - 1) / 2)\n            else:\n                Xab = Xa[a_idx] & Xa[b_idx]\n                for x_idx, y_idx in Xab:\n                    C[a_idx, b_idx] += g3_counts[x_idx, a_idx, y_idx] * g3_counts[x_idx, b_idx, y_idx]\n    NC = cp.sum(C)\n    M = C / NC\n    pa = cp.zeros(n_activities)\n    for a_idx in range(n_activities):\n        pa[a_idx] = M[a_idx, a_idx] + cp.sum(M[a_idx, :]) - M[a_idx, a_idx]\n    E = cp.zeros((n_activities, n_activities))\n    for a_idx in range(n_activities):\n        for b_idx in range(n_activities):\n            if a_idx == b_idx:\n                E[a_idx, b_idx] = pa[a_idx]**2\n            else:\n                E[a_idx, b_idx] = 2 * pa[a_idx] * pa[b_idx]\n    S = cp.log2(M / E)\n    S = cp.where(cp.isinf(S) | cp.isnan(S), 0, S)\n    print(f\"Completed Substitution Scores: {time.time() - start_time:.2f}s\")\n    print(f\"sub_scores: min={cp.min(S):.4f}, max={cp.max(S):.4f}, mean={cp.mean(S):.4f}\")\n    return S\n\ndef compute_insertion_scores_multi_gpu(traces, activities):\n    print(\"Running: Computing Insertion Scores (Algorithm 2)...\")\n    start_time = time.time()\n    n_activities = len(activities)\n    g3_counts = get_3grams(traces, activities)\n    Xa = [set() for _ in range(n_activities)]\n    for x_idx in range(n_activities):\n        for a_idx in range(n_activities):\n            for y_idx in range(n_activities):\n                if g3_counts[x_idx, a_idx, y_idx] > 0:\n                    Xa[a_idx].add((x_idx, y_idx))\n    count_right = cp.zeros((n_activities, n_activities), dtype=cp.float32)\n    for a_idx in range(n_activities):\n        for x_idx in range(n_activities):\n            count_right[a_idx, x_idx] = cp.sum(g3_counts[x_idx, a_idx, :])\n    norm = cp.sum(count_right, axis=1)\n    pa = norm / cp.sum(norm)\n    norm_count_right = count_right / norm[:, cp.newaxis]\n    norm_count_right = cp.where(cp.isnan(norm_count_right), 0, norm_count_right)\n    ins_scores = cp.log2(norm_count_right / (pa[:, cp.newaxis] * pa[cp.newaxis, :]))\n    ins_scores = cp.where(cp.isinf(ins_scores) | cp.isnan(ins_scores), 0, ins_scores)\n    print(f\"Completed Insertion Scores: {time.time() - start_time:.2f}s\")\n    print(f\"ins_scores: min={cp.min(ins_scores):.4f}, max={cp.max(ins_scores):.4f}, mean={cp.mean(ins_scores):.4f}\")\n    return ins_scores\n\n@cuda.jit\ndef edit_distance_kernel(s_batch, t_batch, sub_scores, ins_scores, act_to_idx, distances):\n    i = cuda.grid(1)\n    if i < s_batch.shape[0] * t_batch.shape[0]:\n        s_idx = i // t_batch.shape[0]\n        t_idx = i % t_batch.shape[0]\n        s_len = s_batch[s_idx, 0]\n        t_len = t_batch[t_idx, 0]\n        s = s_batch[s_idx, 1:int(s_len+1)]\n        t = t_batch[t_idx, 1:int(t_len+1)]\n        dp = cuda.local.array((100, 100), dtype=cp.float32)\n        for m in range(int(s_len) + 1):\n            for n in range(int(t_len) + 1):\n                if m == 0 and n == 0:\n                    dp[m, n] = 0\n                elif m == 0:\n                    dp[m, n] = dp[m, n-1] + max(1.0, abs(ins_scores[int(t[n-1]), int(t[n-1])]))\n                elif n == 0:\n                    dp[m, n] = dp[m-1, n] + max(1.0, abs(ins_scores[int(s[m-1]), int(s[m-1])]))\n                else:\n                    sub_cost = 0 if s[m-1] == t[n-1] else max(1.0, abs(sub_scores[int(s[m-1]), int(t[n-1])]))\n                    del_cost = max(1.0, abs(ins_scores[int(s[m-1]), int(s[m-1])]))\n                    ins_cost = max(1.0, abs(ins_scores[int(t[n-1]), int(t[n-1])]))\n                    dp[m, n] = min(dp[m-1, n-1] + sub_cost, dp[m-1, n] + del_cost, dp[m, n-1] + ins_cost)\n        distances[s_idx, t_idx] = dp[int(s_len), int(t_len)]\n\ndef generic_edit_distance_matrix(traces, sub_scores, ins_scores, activities, chunk_size=10000):\n    print(\"Running: Computing Generic Edit Distance matrix on multi-GPU (A4)...\")\n    start_time = time.time()\n    n_chunks = math.ceil(n_traces / chunk_size)\n    max_len = max(max(len(t) for t in traces), 100)\n    act_to_idx = {act: idx for idx, act in enumerate(activities)}\n    traces_padded = cp.zeros((n_traces, max_len + 1), dtype=cp.float32)\n    for i, trace in enumerate(traces):\n        traces_padded[i, 0] = len(trace)\n        for j, act in enumerate(trace):\n            traces_padded[i, j + 1] = act_to_idx[act]\n    sub_scores_gpu = cp.asarray(sub_scores)\n    ins_scores_gpu = cp.asarray(ins_scores)\n\n    def process_chunk(i, gpu_id):\n        with cp.cuda.Device(gpu_id):\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, n_traces)\n            s_batch = traces_padded[start_idx:end_idx]\n            t_batch = traces_padded\n            distances = cp.zeros((s_batch.shape[0], t_batch.shape[0]), dtype=cp.float32)\n            threads_per_block = 256\n            blocks_per_grid = math.ceil((s_batch.shape[0] * t_batch.shape[0]) / threads_per_block)\n            edit_distance_kernel[blocks_per_grid, threads_per_block](\n                s_batch, t_batch, sub_scores_gpu, ins_scores_gpu, cp.array(list(act_to_idx.values())), distances\n            )\n            return distances\n\n    dist_matrix_chunks = []\n    for i in range(n_chunks):\n        dist_matrix_chunks.append(process_chunk(i, i % n_gpus))\n        elapsed = time.time() - start_time\n        progress = (i + 1) / n_chunks\n        eta = elapsed / progress * (1 - progress)\n        print(f\"A4 Distance Matrix: Processed {i+1}/{n_chunks} chunks ({progress*100:.1f}%), ETA: {eta:.2f}s\")\n    \n    dist_matrix = cp.vstack(dist_matrix_chunks)\n    dist_matrix_full = cp.zeros((n_traces, n_traces), dtype=cp.float32)\n    dist_matrix_full[:dist_matrix.shape[0], :] = dist_matrix\n    dist_matrix_full.T[:dist_matrix.shape[0], :] = dist_matrix\n    dist_matrix_full = cp.maximum(dist_matrix_full, 0)\n    print(f\"Completed Distance Matrix: {time.time() - start_time:.2f}s\")\n    return dist_matrix_full.get()\n\n# Phân cụm bằng DBSCAN\ndef cluster_traces(dist_matrix):\n    print(\"Running: DBSCAN clustering...\")\n    start_time = time.time()\n    eps = np.mean(dist_matrix[dist_matrix > 0]) * 0.05\n    clustering = SklearnDBSCAN(eps=eps, min_samples=5, metric=\"precomputed\")\n    labels = clustering.fit_predict(dist_matrix)\n    n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n    print(f\"Completed DBSCAN: {time.time() - start_time:.2f}s (Clusters: {n_clusters})\")\n    return labels, n_clusters\n\n# Tính Silhouette Index\ndef compute_silhouette_index(dist_matrix, labels):\n    print(\"Running: Computing Silhouette Index...\")\n    start_time = time.time()\n    unique_labels = np.unique(labels)\n    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n    if n_clusters < 2:\n        print(\"Warning: Need at least 2 clusters for meaningful Silhouette Index.\")\n        return -1\n    silhouette_scores = []\n    for i in range(len(labels)):\n        if labels[i] == -1: continue\n        same_cluster = (labels == labels[i])\n        a_x = np.mean(dist_matrix[i, same_cluster]) if np.sum(same_cluster) > 1 else 0\n        b_x = float('inf')\n        for label in unique_labels:\n            if label == labels[i] or label == -1: continue\n            other_cluster = (labels == label)\n            if np.sum(other_cluster) > 0:\n                b_x = min(b_x, np.mean(dist_matrix[i, other_cluster]))\n        if a_x == 0 and b_x == 0:\n            s_x = 0\n        else:\n            s_x = (b_x - a_x) / max(a_x, b_x)\n        silhouette_scores.append(s_x)\n    silhouette_avg = np.mean(silhouette_scores) if silhouette_scores else -1\n    print(f\"Completed Silhouette Index: {silhouette_avg:.4f} (Time: {time.time() - start_time:.2f}s)\")\n    return silhouette_avg\n\n# Mã hóa trực quan\ndef visual_encoding(dist_matrix, labels, traces, unique_activities):\n    print(\"Running: Visual Encoding...\")\n    start_time = time.time()\n    \n    # 2.1 MDS\n    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42, normalized_stress='auto')\n    coords = mds.fit_transform(dist_matrix)\n    \n    # 2.2 Giảm chồng lấp\n    cluster_coords = {}\n    for i, label in enumerate(labels):\n        if label not in cluster_coords:\n            cluster_coords[label] = []\n        cluster_coords[label].append(coords[i])\n    adjusted_coords = []\n    for i, (x, y) in enumerate(coords):\n        adjusted_coords.append([x + np.random.uniform(-0.1, 0.1), y + np.random.uniform(-0.1, 0.1)])\n    adjusted_coords = np.array(adjusted_coords)\n    \n    # 2.3 Lấy mẫu dấu vết\n    samples = {}\n    for label in np.unique(labels):\n        if label == -1: continue\n        cluster_traces = [traces[i] for i in range(n_traces) if labels[i] == label]\n        distances = [np.mean([dist_matrix[i, j] for j, l in enumerate(labels) if l == label]) for i in range(n_traces) if labels[i] == label]\n        sorted_idx = np.argsort(distances)\n        percentiles = [0, 25, 50, 75, 100]\n        max_idx = len(sorted_idx) - 1\n        samples[label] = [cluster_traces[sorted_idx[min(int(p * len(sorted_idx) / 100), max_idx)]] for p in percentiles]\n    \n    # 2.4 Sắp xếp dấu vết\n    orders = {}\n    for label in np.unique(labels):\n        if label == -1: continue\n        cluster_traces = [traces[i] for i in range(n_traces) if labels[i] == label]\n        orders[label] = sorted(cluster_traces, key=len)\n    \n    # 2.5 Căn chỉnh dấu vết\n    alignments = {}\n    for label in np.unique(labels):\n        if label == -1: continue\n        cluster_traces = [traces[i] for i in range(n_traces) if labels[i] == label]\n        medoid_idx = np.argmin([np.mean([dist_matrix[i, j] for j, l in enumerate(labels) if l == label]) for i in range(n_traces) if labels[i] == label])\n        medoid = cluster_traces[medoid_idx]\n        aligned = []\n        for trace in cluster_traces:\n            aligned_trace = list(trace)\n            for i, act in enumerate(medoid):\n                if i >= len(trace) or trace[i] != act:\n                    aligned_trace.insert(i, \"-\")\n            aligned.append(tuple(aligned_trace[:len(medoid)]))\n        alignments[label] = aligned\n    \n    # 2.6 Process Graphs\n    process_graphs = {}\n    for label in np.unique(labels):\n        if label == -1: continue\n        cluster_log = pm4py.objects.log.obj.EventLog([trace for i, trace in enumerate(log) if labels[i] == label])\n        net, im, fm = alpha_miner.apply(cluster_log)\n        \n        # Create list of nodes (only transitions)\n        nodes = [t.label for t in net.transitions if t.label is not None]  # Lấy tất cả transitions có nhãn\n        nodes = list(set(nodes))  # Loại bỏ trùng lặp\n        nodes = [{\"id\": str(i), \"label\": node} for i, node in enumerate(nodes)]  # Tạo danh sách nodes với id và label\n        \n        # Create list of edges\n        edges = []\n        edge_counts = Counter()\n        for trace in cluster_log:\n            activities = [event[\"concept:name\"] for event in trace]\n            for i in range(len(activities) - 1):\n                edge = (activities[i], activities[i + 1])\n                edge_counts[edge] += 1\n        \n        # Map activity names to node IDs\n        activity_to_id = {node[\"label\"]: node[\"id\"] for node in nodes}\n        \n        # Convert Petri Net to edges\n        for place in net.places:\n            in_transitions = [arc.source for arc in place.in_arcs if arc.source in net.transitions and arc.source.label is not None]\n            out_transitions = [arc.target for arc in place.out_arcs if arc.target in net.transitions and arc.target.label is not None]\n            for in_t in in_transitions:\n                for out_t in out_transitions:\n                    edge = (in_t.label, out_t.label)\n                    weight = edge_counts.get(edge, 0)\n                    if edge[0] in activity_to_id and edge[1] in activity_to_id:  # Chỉ thêm cạnh nếu cả hai hoạt động đều có trong nodes\n                        edges.append({\n                            \"from\": activity_to_id[edge[0]],\n                            \"to\": activity_to_id[edge[1]],\n                            \"weight\": weight,\n                            \"branch_label\": None\n                        })\n        \n        process_graphs[label] = {\n            \"nodes\": nodes,\n            \"edges\": edges\n        }\n    \n    print(f\"Completed Visual Encoding: {time.time() - start_time:.2f}s\")\n    return adjusted_coords, samples, orders, alignments, process_graphs\n\n# Tạo dữ liệu JSON\ndef create_json_data(labels, coords, samples, orders, alignments, process_graphs, unique_activities, silhouette_score):\n    data = {\n        \"clusters\": {},\n        \"legend\": {act: idx for idx, act in enumerate(unique_activities)},\n        \"cluster_info\": {\n            \"n_clusters\": len(np.unique(labels)) - (1 if -1 in labels else 0),\n            \"rating\": int(silhouette_score * 5)\n        }\n    }\n    for label in np.unique(labels):\n        if label == -1: continue\n        cluster_traces = [traces[i] for i in range(n_traces) if labels[i] == label]\n        activity_counts = Counter([act for trace in cluster_traces for act in trace])\n        total_acts = sum(activity_counts.values())\n        activity_dist = {act: count / total_acts for act, count in activity_counts.items()}\n        performance = [len(trace) for trace in cluster_traces]\n        performance_quartiles = np.percentile(performance, [25, 50, 75, 100])\n        \n        data[\"clusters\"][str(label)] = {\n            \"coords\": coords[labels == label].tolist(),\n            \"samples\": [list(s) for s in samples[label]],\n            \"traces\": [list(t) for t in orders[label]],\n            \"aligned_traces\": [list(t) for t in alignments[label]],\n            \"size\": len([l for l in labels if l == label]),\n            \"activity_distribution\": activity_dist,\n            \"performance\": performance_quartiles.tolist(),\n            \"process_graph\": process_graphs[label]\n        }\n    with open(\"RequestForPayment.json\", \"w\") as f:\n        json.dump(data, f)\n    return data\n\n\nsub_scores = compute_substitution_scores_multi_gpu(traces, unique_activities)\nins_scores = compute_insertion_scores_multi_gpu(traces, unique_activities)\ndist_matrix = generic_edit_distance_matrix(traces, sub_scores, ins_scores, unique_activities)\nlabels, n_clusters = cluster_traces(dist_matrix)\nsilhouette_score = compute_silhouette_index(dist_matrix, labels)\ncoords, samples, orders, alignments, process_graphs = visual_encoding(dist_matrix, labels, traces, unique_activities)\njson_data = create_json_data(labels, coords, samples, orders, alignments, process_graphs, unique_activities, silhouette_score)\nprint(\"JSON data exported to visualization_data.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T03:35:35.372248Z","iopub.execute_input":"2025-05-02T03:35:35.372606Z","iopub.status.idle":"2025-05-02T03:53:44.494199Z","shell.execute_reply.started":"2025-05-02T03:35:35.372581Z","shell.execute_reply":"2025-05-02T03:53:44.492750Z"}},"outputs":[{"name":"stdout","text":"Running: Using 2 GPU(s)...\nRunning: Loading .xes file...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"parsing log, completed traces ::   0%|          | 0/6886 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"146c3dfc681247e1a426f0747d14f1a5"}},"metadata":{}},{"name":"stdout","text":"Completed loading: 2.23s (Traces: 6886)\nRunning: Computing Substitution Scores (Algorithm 1)...\nCompleted Substitution Scores: 1.49s\nsub_scores: min=-11.1204, max=25.2616, mean=0.4545\nRunning: Computing Insertion Scores (Algorithm 2)...\nCompleted Insertion Scores: 1.50s\nins_scores: min=-1.6096, max=19.5603, mean=0.9133\nRunning: Computing Generic Edit Distance matrix on multi-GPU (A4)...\nA4 Distance Matrix: Processed 1/1 chunks (100.0%), ETA: 0.00s\nCompleted Distance Matrix: 1.08s\nRunning: DBSCAN clustering...\nCompleted DBSCAN: 0.86s (Clusters: 33)\nRunning: Computing Silhouette Index...\nCompleted Silhouette Index: 1.0000 (Time: 8.63s)\nRunning: Visual Encoding...\nCompleted Visual Encoding: 1062.81s\nJSON data exported to visualization_data.json\n","output_type":"stream"}],"execution_count":6}]}